import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, accuracy_score
from sklearn.preprocessing import StandardScaler
import seaborn as sns
import matplotlib.pyplot as plt
from statsmodels.stats.outliers_influence import variance_inflation_factor
from statsmodels.api import OLS
from statsmodels.stats.stattools import durbin_watson

# Load the dataset
leads_df = pd.read_csv(r'C:\Users\rajendra.moolya\Downloads\Lead+Scoring+Case+Study\Lead Scoring Assignment\Leads.csv', encoding='unicode_escape')

# Inspect the columns
# print("Columns in the dataset:", leads_df.columns)

# Data Preprocessing
# Handling 'Select' as NaN and filling other NaNs
leads_df.replace('Select', np.nan, inplace=True)
leads_df.fillna(method='ffill', inplace=True)

# Encoding categorical variables using one-hot encoding
non_numeric_columns = leads_df.select_dtypes(include=['object']).columns
leads_df = pd.get_dummies(leads_df, columns=non_numeric_columns, drop_first=True)

# Verify the columns after encoding
# print("Columns after encoding:", leads_df.columns)

# Ensure the required columns are present before dropping
columns_to_drop = ['Prospect ID', 'Lead Number', 'Converted']
columns_to_drop = [col for col in columns_to_drop if col in leads_df.columns]

# Splitting the dataset into features and target variable
X = leads_df.drop(columns=columns_to_drop)
y = leads_df['Converted']

# Ensure all features are numeric
X = X.apply(pd.to_numeric, errors='coerce')

# Drop any rows with NaN values in the features
X.dropna(inplace=True)
y = y.loc[X.index]

# Scaling the features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Splitting the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42)

# Logistic Regression Model
logreg = LogisticRegression(max_iter=1000)
logreg.fit(X_train, y_train)

# Predictions and evaluation for Logistic Regression
y_pred_logreg = logreg.predict(X_test)
accuracy_logreg = accuracy_score(y_test, y_pred_logreg)
report_logreg = classification_report(y_test, y_pred_logreg)

# Random Forest Model
rf_model = RandomForestClassifier(random_state=42)
rf_model.fit(X_train, y_train)

# Predictions and evaluation for Random Forest
y_pred_rf = rf_model.predict(X_test)
accuracy_rf = accuracy_score(y_test, y_pred_rf)
report_rf = classification_report(y_test, y_pred_rf)

# Displaying results
print("Logistic Regression Accuracy:", accuracy_logreg)
print("Logistic Regression Classification Report:\n", report_logreg)

print("Random Forest Accuracy:", accuracy_rf)
print("Random Forest Classification Report:\n", report_rf)

# Analyzing feature importance for Random Forest Model
feature_importances = pd.Series(rf_model.feature_importances_, index=X.columns)
top_features = feature_importances.nlargest(3)

print("Top 3 features contributing significantly to the model:\n", top_features)

# Visualizing the feature importances
top_features.plot(kind='barh')
plt.xlabel('Feature Importance')
plt.ylabel('Features')
plt.title('Top 3 Feature Importances')
plt.show()

# Validating Linear Regression Assumptions (Using Logistic Regression Residuals for demonstration)
residuals = y_test - y_pred_logreg

# Linearity
plt.scatter(y_pred_logreg, residuals)
plt.xlabel('Predicted Values')
plt.ylabel('Residuals')
plt.title('Residuals vs Predicted Values')
plt.show()

# Normality of Residuals
sns.histplot(residuals, kde=True)
plt.title('Distribution of Residuals')
plt.show()

# Q-Q Plot
import scipy.stats as stats
stats.probplot(residuals, dist="norm", plot=plt)
plt.title('Q-Q Plot')
plt.show()

# Homoscedasticity
plt.scatter(y_pred_logreg, residuals)
plt.xlabel('Predicted Values')
plt.ylabel('Residuals')
plt.title('Homoscedasticity Check')
plt.axhline(y=0, color='r', linestyle='--')
plt.show()

# Multicollinearity Check
vif_data = pd.DataFrame()
vif_data["feature"] = X.columns
vif_data["VIF"] = [variance_inflation_factor(X.values, i) for i in range(len(X.columns))]
print(vif_data)

# Durbin-Watson Test for independence
dw_stat = durbin_watson(residuals)
print("Durbin-Watson statistic:", dw_stat)
